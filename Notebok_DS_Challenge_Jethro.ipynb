{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1VT4G5tyl5G"
   },
   "source": [
    "# Kampus Merdeka 6: IBM & Skilvul\n",
    "# Data Science Phase Challenge\n",
    "\n",
    "### Feivel Jethro Ezhekiel | Kelompok 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ6YqGiyyvdC"
   },
   "source": [
    "# Problem Definition\n",
    "## Latar Belakang\n",
    "Semakin hari harga rumah semakin mahal, tetapi keinginan untuk memiliki rumah tetap dimiliki dari berbagai kalangan dari anak muda hingga orang tua. Banyak yang menginginkan rumah yang berada di perkotaan, dekat dengan fasilitas publik, mewah, tetapi harga tetap terjangkau. Oleh karena itu, bank memberikan fitur `Kredit Pemilikan Rumah(KPR)` dimana memiliki rumah terlebih dahulu sembari dicicil. Akan tetapi, pada prosesnya bank tetap memerlukan uji kelayakan dari nasabah yang ingin melakukan KPR, dikarekan banyaknya nasabah yang ingin melakukan KPR tetapi belum tentu memiliki kemampuan untuk mencicil KPR tersebut, maka apabila dilakukan oleh manusia akan sangat terbatas untuk waktu dan tenaga, terutama fokus yang dimiliki untuk `memeriksa` dan `mengevaluasi` nasabah-nasabah tersebut sehingga dibutuhkanlah sebuah teknologi AI yang dapat memprediksi dan memberikan _eligibility rate_ terhadap nasabah-nasabah tersebut.\n",
    "## Tujuan Penelitian\n",
    "Tujuan penelitian ini adalah untuk mementukan ***algoritma*** ML paling *baik* untuk menciptakan teknologi ***AI Eligibility Home Loan Detection*** pada perbankan agar proses verifikasi data lebih cepat dan akurat.\n",
    "## Rumusan Masalah\n",
    "Faktor umur, gaji, banyak orang yang bergantung pada nasabah, dan gender mempengaruhi ***eligibility rate***. \n",
    "## Data yang akan dipakai\n",
    "- Load-test.csv\n",
    "        <br>**Sumber**      : https://www.kaggle.com/datasets/vikasukani/loan-eligible-dataset?select=loan-test.csv\n",
    "        <br>**Deskripsi**   : Dataset untuk testing model dan prediksi dengan data ini, isinya terdapat Loan_ID, Gender(Male/Female), Married(Y/N), Dependents, Education(Graduate/Undergraduate), Self-Employed(Y/N), ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Property_Area(Urban/ Semi-Urban/ Rural), Loan_Status(Y/N). Jumlahnya ada 367 unique values\n",
    "        \n",
    "- Loan-train.csv\n",
    "        <br>**Sumber**      : https://www.kaggle.com/datasets/vikasukani/loan-eligible-dataset?select=loan-train.csv\n",
    "        <br>**Deskripsi**   : Dataset untuk training model dengan dataset berikut dimana isinya terdapat Loan_ID, Gender(Male/Female), Married(Y/N), Dependents, Education(Graduate/Undergraduate), Self-Employed(Y/N), ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History, Property_Area(Urban/ Semi-Urban/ Rural), Loan_Status(Y/N). Jumlanya ada 614 unique values.\n",
    "        \n",
    "## Metode\n",
    "Metode yang digunakan pada penelitian ini menggunakan Supervied Learning Model dimana menggunakan algoritma seperti berikut :\n",
    "* LinearSVC\n",
    "* RandomForest Classifier\n",
    "* KNeighbours Classification\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* XGBoostClassifier\n",
    "* CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5C2jSQ6R98h"
   },
   "source": [
    "# Preparation | Persiapan\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link : https://seaborn.pydata.org/installing.html\n",
    "\n",
    "The basic invocation of pip will install seaborn and, if necessary, its mandatory dependencies. It is possible to include optional dependencies that give access to a few advanced features:\n",
    "\n",
    "!pip install seaborn[stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to install Seaborn - Seaborn is a library for making statistical graphics in Python\n",
    "%pip install seaborn[stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTGMRAdQaHm8"
   },
   "outputs": [],
   "source": [
    "# Initiate libraries that will be used in the process of creating Machine Learning models\n",
    "\n",
    "import pandas as pd # Pandas is a software library written for the Python programming language for data manipulation and analysis\n",
    "import numpy as np # NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these array\n",
    "import matplotlib.pyplot as plt # Basic visualization in python\n",
    "import seaborn as sns # Advanced visualization\n",
    "from sklearn.preprocessing import OneHotEncoder # Encodes categorical data to numerical ones\n",
    "import re # Modificate string data \n",
    "import imblearn # Imblearn library is specifically designed to deal with imbalanced datasets\n",
    "from imblearn.under_sampling import RandomUnderSampler # Balancing data for imbalance in the process\n",
    "from collections import Counter # is a sub-class that is used to count hashable objects. It implicitly creates a hash table of an iterable when invoked\n",
    "from sklearn.model_selection import train_test_split # a powerful tool in Scikit-learn's arsenal, primarily used to divide datasets into training and testing subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPmcajUfSTDd"
   },
   "source": [
    "## Get Data | Mendapatkan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source : https://saturncloud.io/blog/how-to-merge-two-csvs-using-pandas-in-python/\n",
    "\n",
    "# Take training data with a CSV extension into a df variable or Data Frame on the local machine\n",
    "df = pd.read_csv(\"loan-train.csv\")\n",
    "\n",
    "# Retrieve data from the internet\n",
    "# df = request.get(<link>)\n",
    "\n",
    "# Take test data with a CSV extension into the df_validate variable whose function is to test the model at the end of the process\n",
    "df_validate = pd.read_csv('loan-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the contents of the train data on the variable 'df'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the contents of the validate data in the variable 'df_validate'\n",
    "df_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that ```df``` has several features arranged into rows, namely ```Loan_ID, Gender```, ```Married```, ```Dependents```, ``` Education```, ```Self_Employed```, ```ApplicantIncome```, ```CoapplicantIncome```, ```LoanAmount```, ```LoanAmount_Term```, ```Credit History ```, ```Property_Area```, and ```Loan_Status```. \n",
    "\n",
    "* Loan_ ID : customer ID<br>\n",
    "* Gender: Customer's gender<br>\n",
    "* Married: Customer's marital status (Y/N)<br>\n",
    "* Dependents: Number of dependents<br>\n",
    "* Education: Last education (Graduate/Under Graduate)<br>\n",
    "* Self_Employed : Working or not (Y/N)<br>\n",
    "* ApplicantIncome: Customer Income<br>\n",
    "* CoapplicantIncome: Customer Partner's Income<br>\n",
    "* LoanAmount: Loan amount (K/thousand)<br>\n",
    "* Loan_Amount_Term : Loan term in months<br>\n",
    "* Credit_History: Credit history meets guidelines (Y/N)<br>\n",
    "* Loan_Status : Approval Approved(Y/N)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlMrrpF9SaIb"
   },
   "source": [
    "## Explore Data (EDA) | Eksplorasi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we carry out exploration to see and analyze the data we have so that we know the next steps we will take before starting to create a Machine Learning Model.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The main goal of EDA is to gain insight into data and its underlying structure. EDA helps analysts identify patterns, relationships, and outliers in data, which can help in making more informed decisions. EDA can also help identify missing data, errors, and data inconsistencies. By performing EDA, analysts can gain a better understanding of the data, thereby providing more accurate and reliable results.\n",
    "\n",
    "### Importance of EDA for Data Analysis\n",
    "\n",
    "EDA is very important in data analysis because it helps identify patterns, relationships, and anomalies in data. It also helps identify missing data, errors, and data inconsistencies, which can have a significant impact on the analysis. Without EDA, analysts may miss important insights, which can lead to incorrect conclusions and poor decision making. EDA is also important for preparing data for further analysis, such as predictive modeling, machine learning, and statistical inference.\n",
    "\n",
    "reference : https://www.linkedin.com/pulse/power-exploratory-data-analysis-eda-science-basics-best-soni/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_info(df):\n",
    "    # df.shape to see the dimensions of the dataframe\n",
    "    print(f\"Data shape : {df.shape}\")\n",
    "    # df.shape[0] shows many rows, while df.shape[1] shows many columns(features)\n",
    "    print(f\"The data have {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    print()\n",
    "    print(f\"Overall information of the data:\")\n",
    "    print()\n",
    "    print(f\"Large amount of empty data: {df.isnull().sum().sum()}\")\n",
    "    print()\n",
    "    print(df.info(),\"\\n\", df.isnull().sum())# .info() functions to tell the contents of the dataframe and also the type of data contained in it and ensures that the contents are the same as df\n",
    "    print(\"===================================================================\")\n",
    "\n",
    "dataframe_info(df), dataframe_info(df_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat terlihat bahwa pada data training(df) data yang kita miliki memiliki 13 features yang terdiri dari 4 features yang berbentuk float, 1 features yang berbentuk integer, dan 8 features yang berbentuk object maka totalnya adalah 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the original data frame to an empty dataframe which we will then call data_train\n",
    "# The aim is to be more flexible and also aims to be used as training data\n",
    "df_train = df\n",
    "df_test = df_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .describe() Analyze the values ​​contained in each feature using the summary features method\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at it, it turns out that in the 13 features there are only 5 features that are detected as having a numeric value, even though if you look at it there is 1 feature that can be made numeric, namely ```Dependents```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data so that it can be seen well so that it can be communicated with other stakeholders\n",
    "\n",
    "def static_viz(df):\n",
    "    # To set the grid theme on Seaborn\n",
    "    sns.set_theme(style='darkgrid')\n",
    "    # To set the canvas size in Seaborn\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(30,30)) # subplots indicate that there are plots combined in the same canvas which consists of 4 rows and 3 columns\n",
    "    sns.histplot(data = df, x = \"Gender\", color=\"olive\", ax=ax[0, 0])\n",
    "    sns.histplot(data = df, x = \"Married\", color=\"yellowgreen\", ax=ax[0, 1])\n",
    "    sns.histplot(data = df, x = \"Dependents\", color=\"lightcoral\", ax=ax[0, 2])\n",
    "    sns.histplot(data = df, x = \"Education\", color=\"slategray\", ax=ax[1, 0])\n",
    "    sns.histplot(data = df, x = \"Self_Employed\", color=\"violet\", ax=ax[1, 1])\n",
    "    sns.histplot(data = df, x = \"ApplicantIncome\", kde=True, color=\"salmon\", ax=ax[1, 2])\n",
    "    sns.histplot(data = df, x = \"CoapplicantIncome\", kde=True, color=\"lightskyblue\", ax=ax[2, 0])\n",
    "    sns.histplot(data = df, x = \"LoanAmount\", kde=True, color=\"sandybrown\", ax=ax[2, 1])\n",
    "    sns.histplot(data = df, x = \"Loan_Amount_Term\", color=\"paleturquoise\", ax=ax[2, 2])\n",
    "    sns.histplot(data = df, x = \"Credit_History\", color=\"tan\", ax=ax[3, 0])\n",
    "    sns.histplot(data = df, x = \"Property_Area\", color=\"palevioletred\", ax=ax[3, 1])\n",
    "    sns.histplot(data = df, x = \"Loan_Status\", color=\"mediumslateblue\", ax=ax[3, 2])\n",
    "\n",
    "static_viz(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_Viz(df_train, nrows, ncols):\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5)) \n",
    "    \n",
    "    plots = [\n",
    "        (\"Gender\", \"olive\"), (\"Married\", \"yellowgreen\"), (\"Dependents\", \"lightcoral\"),\n",
    "        (\"Education\", \"slategray\"), (\"Self_Employed\", \"violet\"), (\"ApplicantIncome\", \"salmon\"),\n",
    "        (\"CoapplicantIncome\", \"lightskyblue\"), (\"LoanAmount\", \"sandybrown\"),\n",
    "        (\"Loan_Amount_Term\", \"paleturquoise\"), (\"Credit_History\", \"tan\"),\n",
    "        (\"Property_Area\", \"palevioletred\"), (\"Loan_Status\", \"mediumslateblue\")\n",
    "    ]\n",
    "    \n",
    "    for i, (feature, color) in enumerate(plots):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        sns.histplot(data=df_train, x=feature, color=color, ax=ax[row, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "dynamic_Viz(df_train, 4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 12 Features we can see the inequality in the amount of data in the features ```Gender```, ```Married```, ```Dependents```, ```Education```, ```Self_Employed``` , and ```Credit_History``` which impacts ```Loan_Status```. Then in the features ```ApplicantIncome```, ```CoapplicantIncome```, and ```LoanAmount```, there is an abnormal distribution form, aka *skewed-left*.\n",
    "\n",
    "This will greatly influence the final results of our Training Model later. Therefore, we need to clean data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan Correlation Matrix untuk melihat hubungan tiap features\n",
    "\n",
    "sns.heatmap(df_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We cannot see the correlation matrix yet because it contains a feature that cannot be converted into numeric.**\n",
    "\n",
    "**We will do it after data cleansing only**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data(data train) | Membersihkan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ditahap ini kita melakukan pembersihan pada data. Pembersihan ini berupa manipulasi, transformasi, dan juga distribusi normal.\n",
    "\n",
    "1. Manipulasi\n",
    "\n",
    "    Pada manipulasi disini kita melakukan proses penghilangan pada data yang tidak memiliki *value* atau bernilai ```NaN```. Bisa juga memasukan *value* tertentu seperti mean, median, ataupun modus dalam data tersebut.\n",
    "\n",
    "2. Transformasi\n",
    "\n",
    "    Transformasi disini adalah perubahan bentuk data dari satu bentuk ke bentuk lain sesuai dengan kebutuhan dari Machine Learning Model itu sendiri.\n",
    "\n",
    "3. Distribusi\n",
    "\n",
    "    Distribusi disini dilakukan untuk data dapat terdistribusi normal sehingga pemodelannya dapat berjalan dengan **ideal**. Proses ini juga dilakukan dengan menghapus data-data outliers yang terdapat pada dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isnull().sum() berfungsi untuk menghitung jumlah data yang memiliki value NaN di dalamnya di tiap features dalam df_train\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO LIST\n",
    "\n",
    "Based on the data exploration above, we can do the following things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loan ID\n",
    "Loan ID is a Unique Customer Identity which is different for each customer, aka it has no pattern, so this will actually be confusing for the *Machine Learning* model later. Therefore, it is better to just delete this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Loan_ID column because it cannot be predicted by ML with .drop()\n",
    "df_train = df_train.drop('Loan_ID', axis=1) # axis=1 indicates columns\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could do it with the data test also\n",
    "df_test = df_test.drop('Loan_ID', axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gender\n",
    "The Gender feature was simply removed because it is not significant to loan eligibility. This is explained in a paper entitled ```Gender Bias and Credit Access``` which can be accessed at the following link:\n",
    "\n",
    "https://drive.google.com/file/d/1NaboUUhEJALucTLw87R7_sQYhBPmCl88/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed the Gender column because it has no correlation with loan eligibility\n",
    "df_train = df_train.drop('Gender', axis=1) # or you can also use df_train.drop(columns=['Gender'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop('Gender', axis=1) # or you can also use df_test.drop(columns=['Gender'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Married \n",
    "There are 3 empty data lines. These rows of data can be deleted because their number does not have a significant impact on the other 613 data, then these features will be converted to binary form using ``OneHotEncoder```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following article: https://www.rocketlawyer.com/family-and-personal/family-matters/marriage/legal-guide/how-marital-status-affects-credit-card-and-loan-applications. It is stated that marital status influences whether a customer is eligible to borrow money from the bank. Therefore, these *features* will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The syntax below aims to view empty column data in the Married feature so that we can see its contents.\n",
    "df_train[df_train.Married.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.Married.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 3 out of 613 empty data in df_train in the Married feature, so it will not have a significant impact on the ML model. Therefore, we can delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Married column which has the value NaN because it is not significant\n",
    "\n",
    "# dropna is a function to delete data that has NaN values ​​in it only in the 'Married' column \n",
    "df_train.dropna(subset=['Married'], inplace=True)\n",
    "\n",
    "# .isnull().sum() is a function to count the number of empty data in a dataframe\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `Married` feature is in the form of an object or categorical in the form Yes / No, we can transform the data from *object*, to *Boolean* with the OneHotEncoder method, or change the category into 2 binary values, namely 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = df['Married'].head()\n",
    "dum = pd.get_dummies(dum)\n",
    "dum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fast data cleaning and EDA, it makes a lot of sense to use pandas get dummies. However, if I plan to convert a categorical column into multiple binary columns for machine learning, it is better to use OneHotEncoder().\n",
    "\n",
    "https://albertum.medium.com/preprocessing-onehotencoder-vs-pandas-get-dummies-3de1f3d77dcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to binary values ​​using OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output= False).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.fit_transform(y) is the process of learning the existing pattern in y and transforming it into an integer representation according to the index assigned to each unique value in the array y\n",
    "Married_ohe = encoder.fit_transform(df_train[['Married']]).astype(int)\n",
    "Married_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Married_ohe_test = encoder.fit_transform(df_test[['Married']]).astype(int)\n",
    "Married_ohe_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining df Married_ohe with df_train and deleting the old Married column to become a new Married Column with values ​​in the form of binary values\n",
    "df_train = pd.concat([df_train, Married_ohe], axis=1).drop(columns=['Married'])\n",
    "# Married = 1; Not Married = 0\n",
    "df_train = df_train.rename(columns={\"Married_Yes\": \"Married\"})\n",
    "df_train = df_train.drop(columns=['Married_No'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine df Married_ohe_test with df_test and delete the old Married column to become a new Married Column with values ​​in the form of binary values\n",
    "df_test = pd.concat([df_test, Married_ohe_test], axis=1).drop(columns=['Married'])\n",
    "# Married = 1; Not Married = 0\n",
    "df_test = df_test.rename(columns={\"Married_Yes\": \"Married\"})\n",
    "df_test = df_test.drop(columns=['Married_No'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can create a function to combine two steps above\n",
    "\n",
    "def BinaryEncoder(data, features):\n",
    "    for feature in features:\n",
    "        if data[feature].dtype == 'object':\n",
    "            encoder = pd.get_dummies(data[feature], prefix=feature)\n",
    "            data = pd.concat([data, encoder], axis=1)\n",
    "            data.drop(columns=[feature], inplace=True)\n",
    "            data.rename(columns={f\"{feature}_Yes\": feature}, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Dependents (Object)\n",
    "This feature has 15 blanks so we can just drop it even though there are more than Gender, but we also can't manipulate the median or mode because the distribution is? uneven. Then carry out manipulation by removing the ```+``` sign and changing the data form from ```object``` to *numeric values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article: https://www.homeloanexperts.com.au/how-much-can-i-borrow/dependents-on-borrowing-power/\n",
    "\n",
    "It is stated that banks pay attention and consider customers who want to borrow money whether they have children or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the contents of data rows that have NaN values ​​in the Dependents column\n",
    "df_train[df_train.Dependents.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.Dependents.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(1, 3, figsize=(20,15))\n",
    "\n",
    "sns.boxplot(data=df_train, x=\"Dependents\", y=\"LoanAmount\", ax=ax2[0])\n",
    "sns.boxplot(data=df_train, x=\"Dependents\", y=\"ApplicantIncome\", ax=ax2[1])\n",
    "sns.boxplot(data=df_train, x=\"Dependents\", y=\"CoapplicantIncome\", ax=ax2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pattern in the LoanAmount and CoapplicantIncome features where the more dependents, the higher the LoanAmount and the lower the CoapplicantIncome. However, we can carry out further analysis after data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x=\"Dependents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If analysis is carried out, the data rows that have NaN values ​​in them are at the average value, and also have a 50:50 influence on the final results. Therefore, it will not have a significant effect on the Machine Learning Model so we can remove it\n",
    "df_train.dropna(subset=['Dependents'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(subset=['Dependents'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah bentuk 3+ menjadi 3 dan bentuk object menjadi integer\n",
    "df_train['Dependents'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the 610th data contains a ```+``` sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change strings with + signs to disappear : https://saturncloud.io/blog/how-to-remove-special-characters-in-pandas-dataframe/#:~:text=Use%20Regex%20Substitution%3A&text=sub ()%20function%20from%20the,)%2C%20effectively%20removing%20special%20characters.\n",
    "# Change the string in dependents to numeric\n",
    "df_train['Dependents'] = df_train['Dependents'].apply(lambda x:re.sub(r'[/\\+/g]', '', x)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Dependents'] = df_test['Dependents'].apply(lambda x:re.sub(r'[/\\+/g]', '', x)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the + sign has disappeared in the 610th data and also that the data in object form has changed to numeric form so that later the correlation matrix can be carried out\n",
    "df_train[\"Dependents\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"Dependents\"].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Education\n",
    "\n",
    "In *features* ```Education``` there are no empty data rows, i.e. they are all filled. However, the form is still in object form, so we can convert it to a binary value using ```OneHotEncoder```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article: https://www.linkedin.com/pulse/hidden-ways-your-education-level-affects-finances-lana-bandoim/\n",
    "\n",
    "It was found that the level of education influences Loan Eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data form in the Education feature to binary values\n",
    "Education_ohe = encoder.fit_transform(df_train[['Education']]).astype(int)\n",
    "Education_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Education_ohe_test = encoder.fit_transform(df_test[['Education']]).astype(int)\n",
    "Education_ohe_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the initial form before using OneHotEncoder\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Education_ohe with df_train\n",
    "df_train = pd.concat([df_train, Education_ohe], axis=1).drop(columns=['Education'])\n",
    "# Graduate = 1; Not Graduate = 0\n",
    "df_train = df_train.rename(columns={\"Education_Graduate\": \"Education\"})\n",
    "df_train = df_train.drop(columns=[\"Education_Not Graduate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Education_ohe_test with df_test\n",
    "df_test = pd.concat([df_test, Education_ohe_test], axis=1).drop(columns=['Education'])\n",
    "# Graduate = 1; Not Graduate = 0\n",
    "df_test = df_test.rename(columns={\"Education_Graduate\": \"Education\"})\n",
    "df_test = df_test.drop(columns=[\"Education_Not Graduate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat bentuk akhir setelah dilakukan OneHotEncoder\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Self Employed\n",
    "\n",
    "In these Features there are 32 empty rows of data. Then we can convert them to binary values ​​using ```OneHotEncoder```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In several articles it is said that those who are self-employed can take out loans but with several restrictions. However, because here we assume it is general, we will assume it does have an effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the contents of data rows that have NaN values ​​in the Self_Employed column\n",
    "df_train[df_train.Self_Employed.isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax3 = plt.subplots(figsize=(5,10))\n",
    "\n",
    "sns.boxplot(data=df_train, x=\"Self_Employed\", y=\"ApplicantIncome\", hue=\"Loan_Status\", ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the majority of data has the possibility of being accepted by the loan eligibility process and most of it is also Self_Employed, we can fill in the NaN value with the mode value in the feature ```Self_Employed```, namely ```1```, but because the number of Loan_Status is `` `imblance```/ condition where the values ​​```1``` and `0` are not equal in number and will affect the final results of the Machine Learning model, so we can just delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide input with the most information in that column: https://www.makeuseof.com/fill-missing-data-with-pandas/#:~:text=Use%20the%20fillna()%20Method,modal% 2C%20or%20any%20other%20value.\n",
    "# df_train['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(subset=[\"Self_Employed\"], inplace=True)\n",
    "df_train[df_train.Self_Employed.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(subset=[\"Self_Employed\"], inplace=True)\n",
    "df_test[df_test.Self_Employed.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data form in the Self_Employed feature to binary values\n",
    "Self_Employed_ohe = encoder.fit_transform(df_train[['Self_Employed']]).astype(int)\n",
    "Self_Employed_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data form in the Self_Employed feature to binary values\n",
    "Self_Employed_ohe_test = encoder.fit_transform(df_test[['Self_Employed']]).astype(int)\n",
    "Self_Employed_ohe_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Educatio_ohe with df_train\n",
    "df_train = pd.concat([df_train, Self_Employed_ohe], axis=1).drop(columns=['Self_Employed'])\n",
    "# Self_Employed_Yes = 1; Self_Employed_No = 0\n",
    "df_train = df_train.rename(columns={\"Self_Employed_Yes\": \"Self_Employed\"})\n",
    "df_train = df_train.drop(columns=['Self_Employed_No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Educatio_ohe with df_test\n",
    "df_test = pd.concat([df_test, Self_Employed_ohe_test], axis=1).drop(columns=['Self_Employed'])\n",
    "# Self_Employed_Yes = 1; Self_Employed_No = 0\n",
    "df_test = df_test.rename(columns={\"Self_Employed_Yes\": \"Self_Employed\"})\n",
    "df_test = df_test.drop(columns=['Self_Employed_No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Self_Employed\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Applicant Income dan Coapplicant Income\n",
    "In these two features, it is better if we combine them so that it is easier to see the pattern because in the end it is one family who makes the loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no need to question this feature regarding its effect on Loan Status because without income it is impossible to borrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining ApplicantIincome with CoapplicantIncome\n",
    "df_train['Income'] = df_train['ApplicantIncome'] + df_train['CoapplicantIncome']\n",
    "df_train = df_train.drop('ApplicantIncome', axis = 1)\n",
    "df_train = df_train.drop('CoapplicantIncome', axis = 1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining ApplicantIincome with CoapplicantIncome\n",
    "df_test['Income'] = df_test['ApplicantIncome'] + df_test['CoapplicantIncome']\n",
    "df_test = df_test.drop('ApplicantIncome', axis = 1)\n",
    "df_test = df_test.drop('CoapplicantIncome', axis = 1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts float to integer\n",
    "df_train['Income'] = df_train['Income'].astype(int)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts float to integer\n",
    "df_test['Income'] = df_test['Income'].astype(int)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Loan Amount\n",
    "In this feature there are 22 empty rows of data. We need to analyze further because this feature has an important role in the Machine Learning Model, then we also need to change the number of 0s, because in the description the data should be in the form of thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature also has the same effect as Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.LoanAmount.isna()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.LoanAmount.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `Loan_Amount` is important enough to be used in Machine Learning Models, we need to fill in the NaN values, but the NaN values ​​must have the `Loan_Status` N feature so that we can delete the `Loan_Status` with a value of Y and reduce the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_train[df_train.LoanAmount.isna()]\n",
    "df_temp = df_temp((df_temp[\"Loan_Status\"] == 'Y') and df_temp[df_temp.LoanAmount.isna()])\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ternyata kita perlu mengubah data `Loan_Status` menjadi boolean atau numerik, maka kita akan lompat ke tahap ke [12. Loan Status](####-12.-Loan-Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat variabel kosong baru untuk memasukkan baris data yang memiliki NaN value yang memiliki Loan_Status bernilai\n",
    "df_temp = df_train[df_train.LoanAmount.isna()]\n",
    "df_temp = df_temp[df_temp['LoanAmount'].isna() & df_temp['Loan_Status'] == 0]\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus baris data NaN pada Data Frame\n",
    "df_train.dropna(subset=['LoanAmount'], inplace = True)\n",
    "df_train[df_train.LoanAmount.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.drop(df_train.tail(9).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan df_temp diatas dengan df_train[\"LoanAmount\"]\n",
    "df_train = pd.concat([df_train, df_temp])\n",
    "df_train.tail(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi NaN values pada df_temp dengan range antara 9 hingga 177 dikarenakan 570 adalah data outliers yang jumlahnya hanya 1\n",
    "\n",
    "# Kita menggunakan metode polynomial karena seringnya(walaupun tidak selalu), memberikan nilai yang lebih akurat\n",
    "df_train[\"LoanAmount\"] = df_train[\"LoanAmount\"].interpolate(limit_direction='both', method=\"polynomial\", order=2)\n",
    "df_train.tail(10)\n",
    "\n",
    "# reference : https://www.numpyninja.com/post/interpolation-using-pandas#:~:text=Interpolation%20is%20one%20such%20method,series%20while%20pre%2Dprocessing%20data.\n",
    "# https://stackoverflow.com/questions/63632541/order-of-spline-interpolation-for-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sedangkan pada data test set, tidak perlu diutak atik jadi bisa kita drop saja\n",
    "df_test.dropna(subset=['LoanAmount'], inplace=True)\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah tipe data LoanAmount menjadi Integer\n",
    "df_train['LoanAmount'] = df_train['LoanAmount'].astype(int)\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah tipe data LoanAmount menjadi Integer\n",
    "df_test['LoanAmount'] = df_test['LoanAmount'].astype(int)\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambah tiga angka 0 agar menjadi ribuan dan sesuai dengan format awal\n",
    "df_train[\"LoanAmount\"] = df_train[\"LoanAmount\"].mul(1000)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambah tiga angka 0 agar menjadi ribuan dan sesuai dengan format awal\n",
    "df_test[\"LoanAmount\"] = df_test[\"LoanAmount\"].mul(1000)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Loan Amount Term\n",
    "In this feature we can change the form to an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature has an effect on Loan Status, especially on the customer's ability or length of time to repay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Loan_Amount_Term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.Loan_Amount_Term.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.Loan_Amount_Term.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting data that contains NaN because there are only 12 Loan_Amount Terms and half of them are approved and half are not approved so if we delete them it won't be too significant\n",
    "df_train.dropna(subset='Loan_Amount_Term', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(subset='Loan_Amount_Term', inplace=True)\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Loan_Amount_Term'] = df_train['Loan_Amount_Term'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Loan_Amount_Term'] = df_test['Loan_Amount_Term'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Loan_Amount_Term\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 84]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense, if we multiply income by LoanAmountTerm, it won't be able to pay off its debt, so we can delete everything except index 585 because it has Loan Status 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuk menghapus data dengan index tertentu\n",
    "df_train = df_train.drop([313, 495, 575])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still makes sense if we multiply Income by Loan_Amount_Term then he is still able to pay his debts in full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense if we multiply Income by Loan_Amount_Term then he is still able to pay off his debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(df_train[df_train[\"Loan_Amount_Term\"] == 60].index)\n",
    "df_train[df_train[\"Loan_Amount_Term\"] == 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense that if we multiply Income by Loan_Amount_Term then he will still be able to pay off his debt. But because Loan_Status shows 0, we leave it alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 84th data still makes sense, the rest doesn't make sense if we multiply Income by Loan_Amount_Term and the index 591 data status is 0, then we just delete 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop([16, 84])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Loan_Amount_Term\"] == 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense, if we multiply income by LoanAmountTerm, he won't be able to pay off his debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(df_train[df_train[\"Loan_Amount_Term\"] == 12].index)\n",
    "df_train[df_train[\"Loan_Amount_Term\"] == 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Credit History\n",
    "Removed some data containing NaN because Credit History has a significant role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit History has a significant impact on loan Status because that is where the bank can carry out background checking. It will be difficult to see a customer's ability to pay if there is no Credit History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(subset=['Credit_History'], inplace=True)\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[\"Credit_History\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_train, x=\"Credit_History\", y=\"Income\", hue=\"Loan_Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Credit_History\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to balance the credit history of 0 with 1. Then we can fill the credit history of Nan with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Credit_History\"] = df_train[\"Credit_History\"].fillna(0)\n",
    "df_train[\"Credit_History\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train.drop([nomor index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Credit_History'] = df_train['Credit_History'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Credit_History'] = df_test['Credit_History'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Property Area\n",
    "In this feature we only need to change it to `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature also affects Loan Status because if the customer does not pay, the Bank can confiscate the property the customer owns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah bentuk data pada fitur Propery menjadi binary values\n",
    "Property_Area_ohe = encoder.fit_transform(df_train[['Property_Area']]).astype(int)\n",
    "Property_Area_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah bentuk data pada fitur Propery menjadi binary values\n",
    "Property_Area_ohe_test = encoder.fit_transform(df_test[['Property_Area']]).astype(int)\n",
    "Property_Area_ohe_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan Property_Area_ohe menjadi satu\n",
    "df_train = pd.concat([df_train, Property_Area_ohe], axis=1).drop(columns=['Property_Area'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan Property_Area_ohe menjadi satu\n",
    "df_test = pd.concat([df_test, Property_Area_ohe_test], axis=1).drop(columns=['Property_Area'])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 12. Loan Status\n",
    "\n",
    "We just need to convert it to binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features are very important because we will predict the value of these features based on previous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed Loan_Status to OneHotEncoder\n",
    "Loan_Status_ohe = encoder.fit_transform(df_train[['Loan_Status']]).astype(int)\n",
    "Loan_Status_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan_Status condition before OneHotEncoding\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Loan_Status_ohe with df_train\n",
    "df_train = pd.concat([df_train, Loan_Status_ohe], axis=1).drop(columns=['Loan_Status'])\n",
    "# Loan Status Yes = 1; Loan Status No = 0\n",
    "df_train = df_train.rename(columns={\"Loan_Status_Y\": \"Loan_Status\"})\n",
    "df_train = df_train.drop(columns=['Loan_Status_N'])\n",
    "# Loan_Status condition after OneHotEncoding\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan ini kita dapat kembali ke tahap [sebelumnya](####-8.-Loan-Amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### #Standardisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax4 = plt.subplots(ncols=3, figsize=(20,10))\n",
    "sns.histplot(df_train['Income'], bins=30, kde=True, ax=ax4[0])\n",
    "sns.histplot(df_train['LoanAmount'], bins=30, kde=True, ax=ax4[1])\n",
    "sns.histplot(df_train['Loan_Amount_Term'], bins=30, kde=True, ax=ax4[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting data that has a Loan Amount of more than 350000\n",
    "df_train = df_train.drop(df_train[df_train[\"LoanAmount\"] > 350000].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax4 = plt.subplots(ncols=3, figsize=(20,10))\n",
    "sns.histplot(df_train['Income'], bins=30, kde=True, ax=ax4[0])\n",
    "sns.histplot(df_train['LoanAmount'], bins=30, kde=True, ax=ax4[1])\n",
    "sns.histplot(df_train['Loan_Amount_Term'], bins=30, kde=True, ax=ax4[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen in the LoanAmount Feature that the data is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus data income yang outliers agar bisa terstandarizazi\n",
    "df_train = df_train.drop(df_train[df_train[\"Income\"] > 20000].index)\n",
    "fig, ax4 = plt.subplots(ncols=2, figsize=(20,10))\n",
    "sns.histplot(df_train['Income'], bins=30, kde=True, ax=ax4[0])\n",
    "sns.histplot(df_train['LoanAmount'], bins=30, kde=True, ax=ax4[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that when we delete Income, the LoanAmount will have an impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. There is still an Imbalance between Yes and No in Loan Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Loan_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the number is more than 2 times that. So we need to do *under sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Imbalanced-learn library\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalam prosesnya terdapat beberapa metode, yaitu:\n",
    "* Near Miss Undersampling\n",
    "    * NearMiss-1: Majority class examples with minimum average distance to three closest minority class examples.\n",
    "    * NearMiss-2: Majority class examples with minimum average distance to three furthest minority class examples.\n",
    "    * NearMiss-3: Majority class examples with minimum distance to each minority class example.\n",
    "* Condensed Nearest Neighbor Rule Undersampling -> the notion of a consistent subset of a sample set. This is a subset which, when used as a stored reference set for the NN rule, correctly classifies all of the remaining points in the sample set.\n",
    "* Tomek Links for Undersampling -> The condensed nearest-neighbor (CNN) method chooses samples randomly. This results in a)retention of unnecessary samples and b) occasional retention of internal rather than boundary samples.\n",
    "* Random Under Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data X and y data from the dataframe\n",
    "y = df_train['Loan_Status']\n",
    "X = df_train.drop('Loan_Status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=y, data=X)\n",
    "plt.title('Number of Eligible and not Eligible')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler()\n",
    "rus_X_train, rus_y_train = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before sampling class distribution: -\", Counter(y))\n",
    "print(\"Before sampling class distribution: -\", Counter(rus_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_skew = rus_X_train.skew().sort_values(ascending=False)\n",
    "old_skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this article: (https://www.kaggle.com/code/aimack/how-to-handle-skewed-distribution)\n",
    "\n",
    "* Positive values ​​mean skewed-right distribution\n",
    "* Negative means the distribution is skewed-left\n",
    "* 0 means perfect normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax5 = plt.subplots(ncols=2, figsize=(10,5))\n",
    "sns.histplot(rus_X_train['Income'], bins=50, kde=True, ax=ax5[0])\n",
    "sns.histplot(rus_X_train['LoanAmount'], bins=50, kde=True, ax=ax5[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = rus_X_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, rus_y_train], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we don't need to continue with Loan_Amount_Term because the value range is too far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### #Normalisasi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to normalize publish_display_data\n",
    "1. Simple Feature Scaling<br>\n",
    "    ***df['length'] = df['length']/df['length'].max()***\n",
    "2. Min-Max<br>\n",
    "    ***df['length'] = (df['length']-df['length'].min())/(df['length'].max()-df['length'].min())***\n",
    "3. Z-Score<br>\n",
    "    ***df['length'] = (df['length']-df['length'].mean())/df['length'].std()***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple Feature Scaling\n",
    "df_income_SFS = df_train.copy()\n",
    "column1 = 'Income'\n",
    "column2 = 'LoanAmount'\n",
    "column3 = 'Dependents'\n",
    "column4 = 'Loan_Amount_Term'\n",
    "df_income_SFS[column1] = df_income_SFS[column1]/df_income_SFS[column1].max()\n",
    "df_income_SFS[column2] = df_income_SFS[column2]/df_income_SFS[column2].max()\n",
    "df_income_SFS[column3] = df_income_SFS[column3]/df_income_SFS[column3].max()\n",
    "df_income_SFS[column4] = df_income_SFS[column4]/df_income_SFS[column4].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Min-Max\n",
    "df_income_mM = df_train.copy()\n",
    "df_income_mM[column1] = (df_income_mM[column1] - df_income_mM[column1].min())/(df_income_mM[column1].max()-df_income_mM[column1].min())\n",
    "df_income_mM[column2] = (df_income_mM[column2] - df_income_mM[column2].min())/(df_income_mM[column2].max()-df_income_mM[column2].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Z-Score\n",
    "df_income_Z = df_train.copy()\n",
    "df_income_Z[column1] = (df_income_Z[column1] - df_income_Z[column1].mean())/df_income_Z[column1].std()\n",
    "df_income_Z[column2] = (df_income_Z[column2] - df_income_Z[column2].mean())/df_income_Z[column2].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax5 = plt.subplots(ncols=2, nrows=3, figsize=(20,10))\n",
    "\n",
    "sns.histplot(df_income_SFS[column1], bins=50, kde=True, ax=ax5[0, 0])\n",
    "sns.histplot(df_income_SFS[column2], bins=50, kde=True, ax=ax5[0, 1])\n",
    "\n",
    "sns.histplot(df_income_mM[column1], bins=50, kde=True, ax=ax5[1, 0])\n",
    "sns.histplot(df_income_mM[column2], bins=50, kde=True, ax=ax5[1, 1])\n",
    "\n",
    "sns.histplot(df_income_Z[column1], bins=50, kde=True, ax=ax5[2, 0])\n",
    "sns.histplot(df_income_Z[column2], bins=50, kde=True, ax=ax5[2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get to know the unique value in every features\n",
    "for col in rus_X_train.select_dtypes(include=['object', 'bool', 'float64', 'int64', 'int32']).columns:\n",
    "    print(col)\n",
    "    print(rus_X_train[col].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from manual formulas, we can also do this with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_income_normalized = scaler.fit_transform(df_train[['Income']])\n",
    "df_loan_amount_normalized = scaler.fit_transform(df_train[['LoanAmount']])\n",
    "\n",
    "fig, ax6 = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "sns.histplot(df_income_normalized, bins=50, kde=True, ax=ax6[0])\n",
    "sns.histplot(df_loan_amount_normalized, bins=50, kde=True, ax=ax6[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move data that has been normalized and standardized to df_train again\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column1 = 'Income'\n",
    "# column2 = 'LoanAmount'\n",
    "# column3 = 'Dependents'\n",
    "# column4 = 'Loan_Amount_Term'\n",
    "df_train[\"Income\"] = df_income_SFS[column1]\n",
    "df_train[\"LoanAmount\"] = df_income_SFS[column2]\n",
    "df_train[\"Dependents\"] = df_income_SFS[column3]\n",
    "df_train[\"Loan_Amount_Term\"] = df_income_SFS[column4]\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can do matrix correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(df_train.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loan Status is directly correlated with Loan Status and Income is directly correlated with LoanAmount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQks3T8lSofl"
   },
   "source": [
    "# Model Training | Pelatihan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the case that I took, there are several estimators that would be suitable based on the following picture:\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"./ml_map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LinearSVC\n",
    "* RandomForest Classifier\n",
    "* KNeighbours Classification\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "\n",
    "**Extra Experiments**\n",
    "* XGBoost\n",
    "* Catboostclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(seed, estimators, data, clf):\n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Divide the data into features (X) and target (y)\n",
    "    y = data[\"Loan_Status\"]\n",
    "    X = data.drop(\"Loan_Status\", axis=1)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Initialize the estimators\n",
    "    clf = estimators()\n",
    "    \n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate LinearSVC\n",
    "    score = clf.score(X_test, y_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LinearSVC\n",
    "\n",
    "Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main principle of difference between LinearSVC and SVC is as follows:\n",
    "* By *default* scaling, `LinearSVC` minimizes squared hinge loss while SVC minimizes regular hinge `loss`. It is possible to manually specify the string 'hinge' for the `loss` parameter in `LinearSVC`\n",
    "\n",
    "* `LinearSVC` uses One-vs-All (also known as One-vs-Rest) multiclass reduction whereas SVC uses One-vs-One multiclass reduction. This is also noted here. Additionally, for multi-class classification problems, SVC fits the N*(N - 1)/2 model where N is the number of classes. LinearSVC, on the other hand, is only suitable for N models. If the classification problem is binary, then only one model is suitable for both scenarios. multi_class and Decision_function_shape parameters have nothing in common. The second is an aggregator that transforms the results of the decision function in the appropriate form (n_features, n_samples). multi_class is an algorithmic approach to building solutions.\n",
    "\n",
    "* The underlying estimator of `LinearSVC` is liblinear, which in fact imposes a penalty on the intercept. SVC uses the libsvm estimator which does not. The liblinear estimator is optimized for the (special) linear case and thus converges more quickly on large amounts of data than libsvm. That's why `LinearSVC` takes less time to solve the problem.\n",
    "\n",
    "🔗: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(42) # The random function produces the same random or random value every time it is called\n",
    "\n",
    "# Divide the data into 2 variables, features(X) which will be used as parameters and target(y) which will predict the results\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # The goal is to divide the data we have into 2 parts, training data and test.\n",
    "\n",
    "# Inisialisasi LinearSVC\n",
    "clf_LSVC = LinearSVC()\n",
    "\n",
    "# Fit the model\n",
    "clf_LSVC.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating LinearSVC\n",
    "score_LSVC = clf_LSVC.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score_LSVC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = \"clf_LSVC\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, LinearSVC, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LinearSVC with the usual method (without setting hypeparameter tuning and also cross validation) produces a final value of 0.65625."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RandomForestClassifier vs HistoryGradientBoostingClassifier\n",
    "\n",
    "* Random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of a data set and uses an average to improve prediction accuracy and control overfitting\n",
    "\n",
    "* Exact gradient boosting methods that don't scale very well on datasets with a large number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees in the forest use a best split strategy, which is equivalent to passing splitter=\"best\" to the underlying DecisionTreeRegressor. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the entire data set will be used to build each tree.\n",
    "\n",
    "🔗 : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = \"clf_RFC\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, RandomForestClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_RFC = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RandomForestClassifier with the usual method (without setting the hypeparameter tuning and also cross validation) produces a final value of 0.640625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf=\"clf_HGBC\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, HistGradientBoostingClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HistGradientBoostingClassifier is often compared with RandomForestClassifier, especially on small data samples because HGBC is less than optimal on large amounts of data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNeighbours Classification\n",
    "\n",
    "Classifier implementing the k-nearest neighbors vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Neighbors-based classification` is a type of example-based learning or non-generalization learning: it does not attempt to build a general internal model, but only stores examples of training data. Classification is calculated from a simple majority vote of each point's nearest neighbors: a query point is assigned the data class that has the most representatives in the point's nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = \"clf_KNC\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, KNeighborsClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_KNC = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KNeighbors with the usual method (without setting hypeparameter tuning and also cross validation) produces a final value of 0.5625."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Logistic regression is implemented in LogisticRegression. Despite the name, it is implemented as a linear model for classification rather than regression in terms of scikit-learn/ML ​​nomenclature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is also known in the literature as logit regression, maximum entropy classification (MaxEnt) or log-linear classifier. In this model, probabilities that describe the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "🔗 : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = \"clf_LR\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, LogisticRegression, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_LR = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LogisticRegression with the usual method (without setting hypeparameter tuning and also cross validation) produces a final value of 0.625."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree\n",
    "\n",
    "Decision Trees (DT) are non-parametric `supervised-learning` methods used for classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "\n",
    "***Excess***\n",
    "* Simple to understand and interpret. Trees can be visualized.\n",
    "* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created, and empty values ​​need to be removed. Some combinations of trees and algorithms support missing values.\n",
    "* The cost of using the tree (i.e., prediction data) is logarithmic in the number of data points used to train the tree.\n",
    "* Able to handle numeric and categorical data. However, the scikit-learn implementation does not support categorical variables at this time. Other techniques are usually devoted to analyzing data sets that have only one type of variable. See algorithm for more information.\n",
    "* Able to handle multi-output problems.\n",
    "* Uses a white box model. If certain situations can be observed in a model, the explanation of those conditions is easily explained with Boolean logic. In contrast, in black box models (for example, in artificial neural networks), the results may be more difficult to interpret.\n",
    "* Possibility to validate the model using statistical tests. This makes it possible to take into account the reliability of the model.\n",
    "* Performs well even if the assumptions are violated by the actual model on which the data is generated.\n",
    "\n",
    "***Disadvantages of decision trees include:***\n",
    "* Decision tree learners can create trees that are too complex to generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at leaf nodes or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "* Decision trees can be unstable because small variations in the data might produce a completely different tree. This problem is addressed by using decision trees in ensembles.\n",
    "* Decision tree predictions are not smooth or continuous, but are constant estimates bit by bit as seen in the figure above. Therefore, they are not good at extrapolation.\n",
    "* The problem of studying optimal decision trees is known to be NP-complete in some aspects of optimality and even for simple concepts. As a result, practical decision tree learning algorithms are based on heuristic algorithms such as greedy algorithms where locally optimal decisions are made at each node. Such algorithms cannot guarantee to produce globally optimal decision trees. This can be mitigated by training multiple trees in an ensemble learner, where features and samples are randomly sampled with replacement.\n",
    "* There are concepts that are difficult to learn because decision trees cannot express them easily, such as XOR, parity, or multiplexer problems.\n",
    "* Decision tree learner creates a biased tree if some classes dominate. Therefore it is recommended to balance the data set before fitting the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = \"clf_tree\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, tree.DecisionTreeClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_tree = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DecisionTreeClassifier with the usual method (without setting the hypeparameter tuning and also cross validation) produces a final value of 0.640625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuk visualize our Decision tree model\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"Loan_Eligibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import IFrame\n",
    "# filepath = \"Loan_Eligibility.pdf\"\n",
    "# IFrame(filepath, width=120, height=1920)\n",
    "\n",
    "import os\n",
    "path = 'Loan_Eligibility.pdf'\n",
    "os.system(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBoost\n",
    "\n",
    "`XGBoost` is an optimized distributed gradient boosting library designed to be highly `efficient`, `flexible`, and `portable`. It implements machine learning algorithms under the Gradient Boosting framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XGBoost` provides parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate manner. The same code runs on major distributed environments (Hadoop, SGE, MPI) and can solve problems exceeding billions of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The installation, initialization, and optimization processes are contained in the following documentation:\n",
    "https://xgboost.readthedocs.io/en/stable/install.html#python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = \"clf_XGB\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, XGBClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_XGB = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the XGBoostClassifier with the usual method (without setting the hypeparameter tuning and also cross validation) produces a final value of 0.65625."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CatboostClassifier\n",
    "\n",
    "Training and deploying models for classification problems. Provides compatibility with scikit-learn tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is a supervised machine learning method used by the Train Using AutoML tool and uses decision trees for classification and regression. As the name suggests, CatBoost has two main features, it works with categorical data (Cat) and uses gradient boosting (Boost).\n",
    "\n",
    "🔗 : https://catboost.ai/en/docs/features/visualization_jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf = \"clf_CBC\"\n",
    "\n",
    "accuracy = train_and_evaluate(42, CatBoostClassifier, df_train, clf)\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "score_CBC = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CatBoostClassifier with the usual method (without setting hypeparameter tuning and also cross validation) produces a final value of 64.0625%. The cool thing about CatBoostClassifier is that it can be customized with the User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Machine Learning Model\n",
    "\n",
    "There are 3 ways to evaluate Scikit-learn models:\n",
    "\n",
    "    1. Estimator's built-in `score()` method\n",
    "    2. The `scoring()` method\n",
    "    3. Problem-specific metric functions\n",
    "\n",
    "🔗 : https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi dengan metode `score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### • Score\n",
    "\n",
    "This has been done above when conducting model training where the maximum value is 1 and the minimum value is 0.\n",
    "\n",
    "In 'regression' the score functions to find out the coefficient and determination, while in 'classification' it is to find out the mean accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi dengan `Scoring Parameter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Cross Validation\n",
    "\n",
    "Cross Validation is a method where the process of separating training, validation and test data is randomized depending on the parameters we enter in `cv`\n",
    "\n",
    "🔗 : https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Linear SVC\n",
    "clf_LSVC = LinearSVC()\n",
    "clf_LSVC.fit(X_train, y_train)\n",
    "cross_val_score_LSVC = cross_val_score(clf_LSVC, X, y, cv=15)\n",
    "LSVC_cv = np.mean(cross_val_score_LSVC)\n",
    "\n",
    "# Random Forest Classifier\n",
    "clf_RFC = RandomForestClassifier()\n",
    "clf_RFC.fit(X_train, y_train)\n",
    "cross_val_score_RFC = cross_val_score(clf_RFC, X, y, cv=15)\n",
    "RFC_cv = np.mean(cross_val_score_RFC)\n",
    "\n",
    "# KNeighbours Classification\n",
    "clf_KNC = KNeighborsClassifier()\n",
    "clf_KNC.fit(X_train, y_train)\n",
    "cross_val_score_KNC = cross_val_score(clf_KNC, X, y, cv=15)\n",
    "KNC_cv = np.mean(cross_val_score_KNC)\n",
    "\n",
    "# Logistic Regression\n",
    "clf_LR = LogisticRegression()\n",
    "clf_LR.fit(X_train, y_train)\n",
    "cross_val_score_LR = cross_val_score(clf_LR, X, y, cv=15)\n",
    "LR_cv = np.mean(cross_val_score_LR)\n",
    "\n",
    "# Decision Tree\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "cross_val_score_tree = cross_val_score(clf_tree, X, y, cv=15)\n",
    "tree_cv = np.mean(cross_val_score_tree)\n",
    "\n",
    "# XGBoost\n",
    "clf_XGB = XGBClassifier()\n",
    "clf_XGB.fit(X_train, y_train)\n",
    "cross_val_score_XGB = cross_val_score(clf_XGB, X, y, cv=15)\n",
    "XGB_cv = np.mean(cross_val_score_XGB)\n",
    "\n",
    "# CatBoost\n",
    "clf_CBC = CatBoostClassifier()\n",
    "clf_CBC.fit(X_train, y_train)\n",
    "cross_val_score_CBC = cross_val_score(clf_CBC, X, y, cv=15)\n",
    "CBC_cv = np.mean(cross_val_score_CBC)\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "print(f\"Sebelum Cross-Validation : {score_LSVC*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {LSVC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"RandomForestClassifier\")\n",
    "print(f\"Sebelum Cross-Validation : {score_RFC*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {RFC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"KNeighbours\")\n",
    "print(f\"Sebelum Cross-Validation : {score_KNC*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {KNC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Sebelum Cross-Validation : {score_LR*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {LR_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Sebelum Cross-Validation : {score_tree*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {tree_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"XGBoost\")\n",
    "print(f\"Sebelum Cross-Validation : {score_XGB*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {XGB_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"CatBoostClassifier\")\n",
    "print(f\"Sebelum Cross-Validation : {score_CBC*100:.2f}%\")\n",
    "print(f\"Setelah Cross-Validation : {CBC_cv*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# # Function to train classifier, calculate cross-validation scores, and print results\n",
    "# def train_and_print(clf, X_train, y_train, X, y, cv):\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     cross_val_scores = cross_val_score(clf, X, y, cv=cv)\n",
    "#     cv_score = np.mean(cross_val_scores)\n",
    "#     print(f\"Sebelum Cross-Validation: {clf.score(X_train, y_train) * 100:.2f}%\")\n",
    "#     print(f\"Setelah Cross-Validation: {cv_score * 100:.2f}%\")\n",
    "#     print()\n",
    "\n",
    "# # Set random seed\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Split data\n",
    "# y = df_train[\"Loan_Status\"]\n",
    "# X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# # Train and print results for each classifier\n",
    "# classifiers = {\n",
    "#     \"LinearSVC\": LinearSVC(),\n",
    "#     \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "#     \"KNeighbours\": KNeighborsClassifier(),\n",
    "#     \"Logistic Regression\": LogisticRegression(),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(),\n",
    "#     \"XGBoost\": XGBClassifier(),\n",
    "#     \"CatBoostClassifier\": CatBoostClassifier()\n",
    "# }\n",
    "\n",
    "# for name, clf in classifiers.items():\n",
    "#     print(name)\n",
    "#     train_and_print(clf, X_train, y_train, X, y, cv=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi dengan kasus tertentu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi menggunakan Classification Model Evaluatin Metrics\n",
    "\n",
    "1. Accuracy\n",
    "2. Area Under Curve (ROC)/Area Under Curve(AUC)<br>\n",
    "    Comparison of a model's true positive rate(TPR) versus false positive rate(FPR)\n",
    "    * True Positive =  model predicts 1 when truth is 1\n",
    "    * False Positive = model predicts 1 when truth is 0\n",
    "    * True Negative =  model predicts 0 when truth is 0\n",
    "    * False Negative = model predicts 0 when truth is 1\n",
    "3. Confusion Matrix\n",
    "4. Classification report\n",
    "    * Precision\n",
    "    * Recall\n",
    "    * F1_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LinearSVC\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {LSVC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"RandomForestClassifier\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {RFC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"KNeighbours\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {KNC_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {LR_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"Decision Tree\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {tree_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"XGBoost\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {XGB_cv*100:.2f}%\")\n",
    "print(\" \")\n",
    "print(\"CatBoostClassifier\")\n",
    "print(f\"Loan Eligibility Cross-Validation Accuracy: {CBC_cv*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AOC-ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.\n",
    "\n",
    "🔗 : https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5#:~:text=AUC%20%2D%20ROC%20curve%20is%20a,capable%20of%20distinguishing%20between%20classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# LinearSVC tidak mendukung adanya predict_proba dikarenakan diperuntukan untuk multi kelas\n",
    "\n",
    "# Make predictions with probabilities\n",
    "# Random Forest Classifier\n",
    "y_probs_RFC = clf_RFC.predict_proba(X_test)\n",
    "y_probs_positive_RFC = y_probs_RFC[:,1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_RFC, tpr_RFC, thresholds_RFC = roc_curve(y_test, y_probs_positive_RFC)\n",
    "\n",
    "\n",
    "# KNeighbours Classifier\n",
    "y_probs_KNC = clf_KNC.predict_proba(X_test)\n",
    "y_probs_positive_KNC = y_probs_KNC[:, 1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_KNC, tpr_KNC, thresholds_KNC = roc_curve(y_test, y_probs_positive_KNC)\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "y_probs_LR = clf_LR.predict_proba(X_test)\n",
    "y_probs_positive_LR = y_probs_LR[:, 1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_probs_positive_LR)\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "y_probs_tree = clf_tree.predict_proba(X_test)\n",
    "y_probs_positive_tree = y_probs_tree[:, 1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_tree, tpr_tree, thresholds_tree = roc_curve(y_test, y_probs_positive_tree)\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "y_probs_XGB = clf_XGB.predict_proba(X_test)\n",
    "y_probs_positive_XGB = y_probs_XGB[:, 1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_XGB, tpr_XGB, thresholds_XGB = roc_curve(y_test, y_probs_positive_XGB)\n",
    "\n",
    "\n",
    "# CatBoost Classifier\n",
    "y_probs_CBC = clf_CBC.predict_proba(X_test)\n",
    "y_probs_positive_CBC = y_probs_CBC[:, 1]\n",
    "# Calculate fpr, tpr, and thresholds\n",
    "fpr_CBC, tpr_CBC, thresholds_CBC = roc_curve(y_test, y_probs_positive_CBC)\n",
    "\n",
    "\n",
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Membuat fungsi untuk memisualisasikan data\n",
    "def plot_roc_curve(fpr_list, tpr_list, labels):\n",
    "    \"\"\"\n",
    "    Plots multiple ROC curves given lists of false positive rates (fpr) and true positive rates (tpr).\n",
    "    \"\"\"\n",
    "    # Buat canvas untuk multiple plots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "    axes = axes.ravel() # mengubah array multidimensional menjadi array 1 dimensi\n",
    "\n",
    "    # Ploting every ROC Curve\n",
    "    for fpr, tpr, label, ax in zip(fpr_list, tpr_list, labels, axes): # digunakan untuk menggabungkan dua atau lebih iterables menjadi satu, dengan menghasilkan tuple yang berisi elemen-elemen yang sesuai dari setiap iterable\n",
    "        # Plot roc curve\n",
    "        ax.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "        # Plot line with no predictive power (baseline)\n",
    "        ax.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Guessing\")\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_xlabel(\"False positive rate (fpr)\")\n",
    "        ax.set_ylabel(\"True positive rate (tpr)\")\n",
    "        ax.set_title(f\"ROC Curve for {label}\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate ROC curves\n",
    "classifiers = {\n",
    "    \"Random Forest Classifier\": clf_RFC,\n",
    "    \"KNeighbours Classifier\": clf_KNC,\n",
    "    \"Logistic Regression\": clf_LR,\n",
    "    \"Decision Tree\": clf_tree,\n",
    "    \"XGBoost\": clf_XGB,\n",
    "    \"CatBoost Classifier\": clf_CBC\n",
    "}\n",
    "\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "labels = []\n",
    "\n",
    "for label, clf in classifiers.items():\n",
    "    # Make predictions with probabilities\n",
    "    y_probs = clf.predict_proba(X_test)\n",
    "    y_probs_positive = y_probs[:, 1]\n",
    "    # Calculate fpr, tpr, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)\n",
    "    labels.append(label)\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curve(fpr_list, tpr_list, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "cara tercepat untuk memprediksi label yang model prediksi dengan label sebenarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan cara manual\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds_CBC = clf_CBC.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds_CBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn juga menyediakan fitur ini\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds_CBC = clf_CBC.predict(X_test)\n",
    "\n",
    "# set the font scale\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test, y_preds_CBC)\n",
    "\n",
    "# Plot it using seaborn\n",
    "sns.heatmap(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def display_confusion_matrices(estimators, X_list, y_list):\n",
    "    \"\"\"\n",
    "    Display confusion matrices for multiple classifiers in subplots.\n",
    "\n",
    "    Parameters:\n",
    "        - estimators: List of classifier estimators.\n",
    "        - X_list: List of feature matrices.\n",
    "        - y_list: List of label vectors.\n",
    "    \"\"\"\n",
    "    num_estimators = len(estimators)\n",
    "    num_rows = (num_estimators + 1) // 2  # Adjust for odd number of classifiers\n",
    "    num_cols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6*num_rows))\n",
    "\n",
    "    for idx, (estimator, X, y) in enumerate(zip(estimators, X_list, y_list)):\n",
    "        row = idx // num_cols\n",
    "        col = idx % num_cols\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = estimator.predict(X)\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "        # Display confusion matrix\n",
    "        ax = axes[row, col]\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(ax=ax)\n",
    "        ax.set_title(f'Confusion Matrix - {type(estimator).__name__}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "estimators = [clf_LSVC, clf_RFC, clf_KNC, clf_LR, clf_tree, clf_XGB, clf_CBC]\n",
    "X_list = [X] * len(estimators)  # Assuming same features for all classifiers\n",
    "y_list = [y] * len(estimators)  # Assuming same labels for all classifiers\n",
    "\n",
    "display_confusion_matrices(estimators, X_list, y_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_classification_reports(estimators, X, y):\n",
    "    \"\"\"\n",
    "    Generate classification reports for multiple classifiers.\n",
    "\n",
    "    Parameters:\n",
    "        - estimators: List of classifier estimators.\n",
    "        - X: Feature matrix.\n",
    "        - y: Label vector.\n",
    "    \"\"\"\n",
    "    for estimator in estimators:\n",
    "        # Make predictions\n",
    "        y_pred = estimator.predict(X)\n",
    "        # Generate classification report\n",
    "        report = classification_report(y, y_pred)\n",
    "        # Print classification report\n",
    "        print(f\"Classification Report - {type(estimator).__name__}:\\n{report}\\n\")\n",
    "\n",
    "# Run the classification report\n",
    "\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "estimators = [clf_LSVC, clf_RFC, clf_KNC, clf_LR, clf_tree, clf_XGB, clf_CBC]\n",
    "\n",
    "generate_classification_reports(estimators, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model\n",
    "\n",
    "Kita perlu meningkatkan performa ML model kita dengan mengatur `parameter` dan `Hyper Parameter` *tuning*. Usaha ini dilakukan dengan mengganti parameter pada masing-masing model agar mendapatkan performa yang optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hyperparameters` vs `Parameters`\n",
    "\n",
    "* Parameters = model find these patterns in data\n",
    "* Hyperparameters = settings on a model you can adjust to (potentially) improve its ability to find patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "clf_LSVC = LinearSVC(C=20,\n",
    "                    max_iter=1000,\n",
    "                    verbose=25)\n",
    "\n",
    "clf_LSVC.fit(X_train, y_train)\n",
    "\n",
    "score = clf_LSVC.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada LinearSVC mengalami peningkatan ketika C kita ubah ke 20 dan verbose kita ganti ke 25 hingga mengalami peningkatan akurasi hingga 0.703125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuk melihat list hyper parameternya\n",
    "clf_LSVC.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi RandomForestClassifier\n",
    "clf_RFC = RandomForestClassifier(criterion=\"gini\",\n",
    "                                max_depth=15,\n",
    "                                max_leaf_nodes=50,\n",
    "                                n_estimators=20)\n",
    "\n",
    "# Fit the model\n",
    "clf_RFC.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi RandomForestClassifier\n",
    "score = clf_RFC.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada RandomForestClassifier mengalami peningkatan ketika mengatur max_depth, max_leaf_nodes, dan n_estimators hingga mengalami peningkatan akurasi hingga 73.44%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RFC.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. KNeighbours Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi KNeighborsClassifier\n",
    "clf_KNC = KNeighborsClassifier(n_neighbors=20,\n",
    "                                p=1)\n",
    "\n",
    "# Fit the model\n",
    "clf_KNC.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi KNeighborsClassifier\n",
    "score = clf_KNC.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada KNeighbours Classification mengalami peningkatan ketika mengatur n_neighbors dan p hingga mengalami peningkatan akurasi hingga 67.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_KNC.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Mengatur Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi LogisticRegression\n",
    "clf_LR = LogisticRegression(solver=\"saga\",\n",
    "                            penalty=\"elasticnet\",\n",
    "                            l1_ratio=0.3,\n",
    "                            n_jobs=20,\n",
    "                            max_iter=100,\n",
    "                            random_state=7)\n",
    "\n",
    "# Fit the model\n",
    "clf_LR.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi model\n",
    "score = clf_LR.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengatur hyperparameter tuning tidak memberikan dampak sinifikan pada hasil akhir Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Mengatur Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi tree\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                                splitter='best',\n",
    "                                max_depth=2, \n",
    "                                max_features=4,\n",
    "                                random_state=42\n",
    "                                )\n",
    "\n",
    "# Fit the model\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi model\n",
    "score = clf_tree.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada DecisionTree mengalami peningkatan ketika mengatur criterion, splitter, max_depth, dan max_features hingga mengalami peningkatan akurasi hingga 67.19%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Mengatur Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi XGBClassifier\n",
    "clf_XGB = XGBClassifier(booster=\"gbtree\",\n",
    "                        device=\"gpu\",\n",
    "                        max_depth=100,\n",
    "                        n_estimators=100,\n",
    "                        n_jobs=20,\n",
    "                        num_parallel_tree=10,\n",
    "                        tree_method=\"auto\")\n",
    "\n",
    "# Fit the model\n",
    "clf_XGB.fit(X_train, y_train)\n",
    "\n",
    "# Mengevaluasi model\n",
    "score = clf_XGB.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada XGBoostClassifier mengalami peningkatan ketika mengatur criterion, splitter, max_depth, dan max_features hingga mengalami peningkatan akurasi hingga 64.06% dari 59.38%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Mengatur Random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi CatBoostClassifier\n",
    "clf_CBC = CatBoostClassifier(max_depth=3, verbose=None)\n",
    "\n",
    "# Fit the model\n",
    "clf_CBC.fit(X_train, y_train, plot=True)\n",
    "\n",
    "# Mengevaluasi model\n",
    "score = clf_CBC.score(X_test, y_test)\n",
    "print(f\"Mean Accuracy : {score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada CatBoostClassifier mengalami peningkatan ketika mengatur criterion, splitter, max_depth, dan max_features hingga mengalami peningkatan akurasi hingga 67.19% dari 60.94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostClassifier(\n",
    "    # iterations=None,\n",
    "    # learning_rate=None,\n",
    "    # depth=None,\n",
    "    # l2_leaf_reg=None,\n",
    "    # model_size_reg=None,...\n",
    "    # max_depth=None,\n",
    "    # n_estimators=None,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We want to create train, validation, and test sets\n",
    "\n",
    "* Train set for training purpose\n",
    "* Validation set for hyperparameters get tuned\n",
    "* Test set for model evaluation purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# First of all we create function to evaluate our models\n",
    "\n",
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_preds labels on a classification.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                    \"precision\": round(precision, 2),\n",
    "                    \"recall\": round(recall, 2),\n",
    "                    \"f1\": round(f1, 2)}\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create train, validation, and test sets\n",
    "# Train set for training purpose\n",
    "# Validation set for hyperparameters get tuned\n",
    "# Test set for model evaluation purpose\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split the data into train, validation, & test sets\n",
    "train_split = round(0.7 * len(df_train_shuffled)) # 70% of the data\n",
    "valid_split = round(train_split + 0.15 * len(df_train_shuffled)) # 15% of the data\n",
    "X_train, y_train =X[:train_split], y[:train_split]\n",
    "X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]\n",
    "X_test, y_test = X[valid_split:], y[valid_split:]\n",
    "\n",
    "len(X_train), len(X_valid), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LSVC.fit(X_train, y_train)\n",
    "clf_RFC.fit(X_train, y_train)\n",
    "clf_KNC.fit(X_train, y_train)\n",
    "clf_LR.fit(X_train, y_train)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "clf_XGB.fit(X_train, y_train)\n",
    "clf_CBC.fit(X_train, y_train)\n",
    "\n",
    "# Make baseline predictions\n",
    "y_preds_LSVC = clf_LSVC.predict(X_valid)\n",
    "y_preds_RFC = clf_RFC.predict(X_valid)\n",
    "y_preds_KNC = clf_KNC.predict(X_valid)\n",
    "y_preds_LR = clf_LR.predict(X_valid)\n",
    "y_preds_tree = clf_tree.predict(X_valid)\n",
    "y_preds_XGB = clf_XGB.predict(X_valid)\n",
    "y_preds_CBC = clf_CBC.predict(X_valid)\n",
    "\n",
    "# Evaluate the classifier on validation set\n",
    "print(\" \")\n",
    "print(\"LinearSVC Classifier\")\n",
    "baseline_metrics_LSVC = evaluate_preds(y_valid, y_preds_LSVC)\n",
    "print(\" \")\n",
    "print(\"Random Forest Classifier\")\n",
    "baseline_metrics_RFC = evaluate_preds(y_valid, y_preds_RFC)\n",
    "print(\" \")\n",
    "print(\"KNeighbours Classifier\")\n",
    "baseline_metrics_KNC = evaluate_preds(y_valid, y_preds_KNC)\n",
    "print(\" \")\n",
    "print(\"Logistic Regression Classifier\")\n",
    "baseline_metrics_LR = evaluate_preds(y_valid, y_preds_LR)\n",
    "print(\" \")\n",
    "print(\"Decision Tree Classifier\")\n",
    "baseline_metrics_tree = evaluate_preds(y_valid, y_preds_LSVC)\n",
    "print(\" \")\n",
    "print(\"XGBoost Classifier\")\n",
    "baseline_metrics_XGB = evaluate_preds(y_valid, y_preds_XGB)\n",
    "print(\" \")\n",
    "print(\"CatBoost Classifier\")\n",
    "baseline_metrics_CBC = evaluate_preds(y_valid, y_preds_CBC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dikarenakan selama ini kita menggunakan metode manual, atau mencoba satu-satu, ini akan memakan banyak waktu dan tenaga. Maka Sklearn menciptakan sebuah library untuk mencari hyperparameter terbaik secara otomatis, yaitu `GridSearch` dan `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid = {\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\", None],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf_RFC = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf_RFC = RandomizedSearchCV(estimator=clf_RFC,\n",
    "                                param_distributions=grid,\n",
    "                                n_iter=50, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "rs_clf_RFC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_RFC.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dari hasil diatas, maka kita bisa mendapatkan best parameter tanpa perlu mencoba satu persatu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds = rs_clf_RFC.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics = evaluate_preds(y_test, rs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terlihat bahwa semakin kita atur RandomizedSearchCV, maka semakin meningkat score nya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {\n",
    "     'n_estimators': [500, 1000, 1200],\n",
    "     'max_depth': [None, 5],\n",
    "     'max_features': ['sqrt'],\n",
    "     'min_samples_split': [6],\n",
    "     'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf_RFC = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_RFC = GridSearchCV(estimator=clf_RFC,\n",
    "                                param_grid=grid_2,\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_RFC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_RFC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_clf_RFC.predict(X_test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare different models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\n",
    "    \"baseline\": baseline_metrics_RFC,\n",
    "    \"random search\": rs_metrics,\n",
    "    \"grid search\": gs_metrics\n",
    "})\n",
    "\n",
    "compare_metrics.plot.bar(figsize=(10,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def compare(baseline_metrics, rs_metrics, gs_metrics, title):\n",
    "    compare_metrics = pd.DataFrame({\n",
    "        \"baseline\": baseline_metrics,\n",
    "        \"random search\": rs_metrics,\n",
    "        \"grid search\": gs_metrics\n",
    "    })\n",
    "    ax = compare_metrics.plot.bar(figsize=(10,8))\n",
    "    ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kesimpulan :\n",
    "\n",
    "Ketika melatih model kita gunakan Train set\n",
    "Ketika ingin memvalidasi model gunakan Validation set\n",
    "Ketika sudah mantab dengan model yang dimiliki, barulah gunakan Test set untuk menguji performa\n",
    "\n",
    "Perbedaan RandomizedSearchCV(RSCV) dengan GridSearchCV(GSCV) adalah RSCV mencari secara acak sedangkan GSCV mencoba semua kemungkinan yang ada\n",
    "\n",
    "Maka kita dapat mencari hyperparameter menggunakan RSCV terlebih dahulu, ketika sudah ditemukan hyperparameter yang lebih kerucut, barulah kita bisa gunakan GSCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan pemisahan data set menjadi training, validation, dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# # Split the data into train, validation, & test sets\n",
    "# train_split = round(0.7 * len(df_train_shuffled)) # 70% of the data\n",
    "# valid_split = round(train_split + 0.15 * len(df_train_shuffled)) # 15% of the data\n",
    "# X_train, y_train =X[:train_split], y[:train_split] # Fit the model\n",
    "# X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split] # Tune the hyperparameter\n",
    "# X_test, y_test = X[valid_split:], y[valid_split:] # Evaluate using metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_train_evaluate(X_train, y_train, X_test, y_test, classifier, estimators):\n",
    "    \"\"\"\n",
    "    To predict using baseline models \n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Train/Fit the models\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Create the baseline predictions\n",
    "    y_preds_test = classifier.predict(X_test)\n",
    "    print(\" \")\n",
    "    print(\"Baseline\", estimators, \"classifier performance: \")\n",
    "    baseline_test = evaluate_preds(y_test, y_preds_test)\n",
    "\n",
    "    return baseline_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Improved LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Inisialisasi LinearSVC\n",
    "clf_LSVC = LinearSVC(dual=True, random_state=42)\n",
    "\n",
    "baseline_LSVC = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_LSVC, estimators = \"LinearSVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LSVC.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_LSVC ={\n",
    "    \"C\": [0.1, 1, 10, 100, 1000],\n",
    "    \"max_iter\": [100, 200, 300, 400, 500, 700, 800, 1000],\n",
    "    \"dual\":[\"auto\"],\n",
    "    \"loss\":[\"hinge\", \"squared_hinge\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_LSVC = RandomizedSearchCV(estimator=clf_LSVC,\n",
    "                                param_distributions=grid_LSVC,\n",
    "                                n_iter=50, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_LSVC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_LSVC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_LSVC = rs_clf_LSVC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_LSVC = evaluate_preds(y_valid, rs_y_preds_LSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_LSVC_2 ={\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"dual\":[\"auto\"],\n",
    "    \"loss\":[\"hinge\", \"squared_hinge\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GridSearchCV\n",
    "gs_clf_LSVC = GridSearchCV(estimator=clf_LSVC,\n",
    "                            param_grid=grid_LSVC_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_LSVC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_LSVC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_LSVC = gs_clf_LSVC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_LSVC = evaluate_preds(y_valid, gs_y_preds_LSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_LSVC, rs_metrics_LSVC, gs_metrics_LSVC, title=\"Comparison of LinearSVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Improved RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf_RFC = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "baseline_RFC = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_RFC, estimators = \"RandomForest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RFC.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_RFC ={\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"sqrt\", None],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\":[\"gini\", \"entropy\", \"log_loss\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_RFC = RandomizedSearchCV(estimator=clf_RFC,\n",
    "                                param_distributions=grid_RFC,\n",
    "                                n_iter=50, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_RFC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_RFC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_RFC = rs_clf_RFC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_RFC = evaluate_preds(y_valid, rs_y_preds_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_RFC_2 ={\n",
    "    'n_estimators': [500, 1000, 1200],\n",
    "     'max_depth': [None, 5],\n",
    "     'max_features': ['sqrt'],\n",
    "     'min_samples_split': [6],\n",
    "     'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "y = df_train[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf_RFC = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_RFC = GridSearchCV(estimator=clf_RFC,\n",
    "                                param_grid=grid_2,\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_RFC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_RFC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_RFC = gs_clf_RFC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_RFC = evaluate_preds(y_valid, gs_y_preds_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_RFC, rs_metrics_RFC, gs_metrics_RFC, title=\"Comparison of RandomForest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Improved KNeighbors Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline KNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi KNeighborsClassifier\n",
    "clf_KNC = KNeighborsClassifier(n_jobs=-1)\n",
    "# Fit the model\n",
    "clf_KNC.fit(X_train, y_train)\n",
    "\n",
    "baseline_KNC = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_KNC, estimators = \"KNeighbours Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_KNC.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNC = {\n",
    "    'n_neighbors': [1, 5, 10, 90],\n",
    "    'leaf_size': [10, 20, 30 ,40, 50, 100, 1200],\n",
    "    'p': [1,2],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['minkowski', 'chebyshev'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_KNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_KNC = RandomizedSearchCV(estimator=clf_KNC,\n",
    "                                param_distributions=grid_KNC,\n",
    "                                n_iter=10, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_KNC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_KNC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_KNC = rs_clf_KNC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_KNC = evaluate_preds(y_valid, rs_y_preds_KNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_KNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_KNC_2 = {\n",
    "    'n_neighbors': [90],\n",
    "    'leaf_size': [2, 10, 30],\n",
    "    'p': [1,2],\n",
    "    'weights': ['distance'],\n",
    "    'metric': ['minkowski'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate KNeigbors Classifier\n",
    "clf_KNC = KNeighborsClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_KNC = GridSearchCV(estimator=clf_KNC,\n",
    "                            param_grid=grid_KNC_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_KNC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_KNC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_KNC = gs_clf_KNC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_KNC = evaluate_preds(y_valid, gs_y_preds_KNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_KNC, rs_metrics_KNC, gs_metrics_KNC, title=\"Comparison of KNeighbours Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improved Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop(\"Loan_Status\", axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "clf_LR = LogisticRegression()\n",
    "\n",
    "clf_LR.fit(X_train, y_train)\n",
    "# Create the baseline predictions\n",
    "y_preds_test = clf_LR.predict(X_test)\n",
    "print(\"Baseline Logistic Regression classifier performance: \")\n",
    "baseline_LR = evaluate_preds(y_test, y_preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LR.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_LR = {\n",
    "    \"C\": [1.0, 1.5, 2.0, 2.5, 3.0],\n",
    "    \"max_iter\": [1, 5, 10, 100, 200, 500, 1000, 1200],\n",
    "    \"solver\": [\"newton-cholesky\", \"liblinear\"],\n",
    "    \"n_jobs\":[-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_LR = RandomizedSearchCV(estimator=clf_LR,\n",
    "                                param_distributions=grid_LR,\n",
    "                                n_iter=80, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_LR.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_LR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_LR = rs_clf_LR.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_LR = evaluate_preds(y_valid, rs_y_preds_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_LR_2 = {\n",
    "    \"C\": [3.0, 4.0, 5.0],\n",
    "    \"max_iter\": [1, 2, 3, 4, 5],\n",
    "    \"solver\": [\"newton-cholesky\"],\n",
    "    \"n_jobs\":[-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_LR = GridSearchCV(estimator=clf_LR,\n",
    "                            param_grid=grid_LR_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_LR.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_LR.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_LR = gs_clf_LR.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_LR = evaluate_preds(y_valid, gs_y_preds_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_LR, rs_metrics_LR, gs_metrics_LR, title=\"Comparison of Logistic Regression Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Improved Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi KNeighborsClassifier\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "# Fit the model\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "baseline_tree = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_tree, estimators = \"Decision Tree Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree = {\n",
    "    \"splitter\" : [\"best\", \"random\"],\n",
    "    \"min_samples_split\":[2, 10, 100, 200, 500, 1200],\n",
    "    \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "    \"max_leaf_nodes\" : [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_tree = RandomizedSearchCV(estimator=clf_tree,\n",
    "                                param_distributions=grid_tree,\n",
    "                                n_iter=10, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_tree.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_tree = rs_clf_tree.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_tree = evaluate_preds(y_valid, rs_y_preds_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree_2 = {\n",
    "    \"splitter\" : [\"best\"],\n",
    "    \"min_samples_split\":[100, 200,500, 600, 1000, 2000],\n",
    "    \"max_features\":[\"sqrt\", \"log2\", None],\n",
    "    \"max_leaf_nodes\":[None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_tree = GridSearchCV(estimator=clf_tree,\n",
    "                            param_grid=grid_tree_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_tree.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_tree = gs_clf_tree.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_tree = evaluate_preds(y_valid, gs_y_preds_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_tree, rs_metrics_tree, gs_metrics_tree, title=\"Comparison of Decision Tree Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improved XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi KNeighborsClassifier\n",
    "clf_XGB = XGBClassifier()\n",
    "# Fit the model\n",
    "clf_XGB.fit(X_train, y_train)\n",
    "\n",
    "baseline_XGB = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_XGB, estimators = \"XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB = {\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    \"n_estimators\": [10, 100, 500, 800, 1000],\n",
    "    \"max_depth\": [2, 3, 5, 7, 10, 100],\n",
    "    \"min_child_weight\": [5, 10, 20, 40, 80, 100],\n",
    "    \"subsample\": [0.5, 0.7, 1.0],\n",
    "    \"colsample_bytree\": [0.1, 0.01, 0.001],\n",
    "    \"objective\": [\"binary:logistic\"],\n",
    "    \"scale_pos_weight\":[1],\n",
    "    \"device\":[\"cpu\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_XGB = RandomizedSearchCV(estimator=clf_XGB,\n",
    "                                param_distributions=grid_XGB,\n",
    "                                n_iter=50, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_XGB.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_XGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_XGB = rs_clf_XGB.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_XGB = evaluate_preds(y_valid, rs_y_preds_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_XGB_2 = {\n",
    "    \"learning_rate\": [0.01, 0.001],\n",
    "    \"n_estimators\": [500, 800, 1000, 1200, 1500],\n",
    "    \"max_depth\": [5, 7],\n",
    "    \"min_child_weight\": [10, 20, 40, 80],\n",
    "    \"subsample\": [1.0],\n",
    "    \"colsample_bytree\": [0.001],\n",
    "    \"objective\": [\"binary:logistic\"],\n",
    "    \"scale_pos_weight\":[1],\n",
    "    \"device\":[\"cpu\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_XGB = GridSearchCV(estimator=clf_XGB,\n",
    "                            param_grid=grid_XGB_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_XGB.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_XGB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_XGB = gs_clf_XGB.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_XGB = evaluate_preds(y_valid, gs_y_preds_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_XGB, rs_metrics_XGB, gs_metrics_XGB, title=\"Comparison of XGBoost Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Improved CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "# Mengatur random seed\n",
    "np.random.seed(42) # Fungsi acak untuk menghasilkan nilai acak atau random yang sama setiap kali dipanggil\n",
    "\n",
    "# Membagi data menjadi 2 variabel, features(X) yang akan digunakan sebagai parameter dan target(y) yang akan diprediksi hasilnya\n",
    "y = df_train[\"Loan_Status\"]\n",
    "X = df_train.drop(\"Loan_Status\", axis=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) # Tujuannya untuk membagi data yang kita punya menjadi 3 bagian, data training, validation, dan test.\n",
    "\n",
    "# Inisialisasi KNeighborsClassifier\n",
    "clf_CBC = CatBoostClassifier(verbose=0)\n",
    "# Fit the model\n",
    "clf_CBC.fit(X_train, y_train)\n",
    "\n",
    "baseline_CBC = baseline_train_evaluate(X_train, y_train, X_test, y_test, clf_CBC, estimators = \"CatBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = CatBoostClassifier.__doc__\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_CBC = {\n",
    "    \"iterations\": [500, 700, 900, 1200],\n",
    "    \"learning_rate\": [0.03, 0.5, 0.7, 1.0],\n",
    "    \"depth\":[1, 3, 5, 7],\n",
    "    \"leaf_estimation_iterations\": [10],\n",
    "    \"leaf_estimation_method\": [\"Newton\", \"Gradient\"],\n",
    "    \"thread_count\":[-1],\n",
    "    \"verbose\":[0],\n",
    "    \"task_type\":[\"CPU\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rs_CatBoosterClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup RandomizedSearchCV to search the best parameters\n",
    "rs_clf_CBC = RandomizedSearchCV(estimator=clf_CBC,\n",
    "                                param_distributions=grid_CBC,\n",
    "                                n_iter=10, # Number of models to try\n",
    "                                cv=5,\n",
    "                                verbose=2)\n",
    "\n",
    "rs_clf_CBC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf_CBC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "rs_y_preds_CBC = rs_clf_CBC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "rs_metrics_CBC = evaluate_preds(y_valid, rs_y_preds_CBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gs_CatBoosterClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_CBC_2 ={\n",
    "    \"iterations\": [500, 600, 700],\n",
    "    \"learning_rate\": [0.03, 0.5],\n",
    "    \"depth\":[1, 3],\n",
    "    \"leaf_estimation_iterations\": [10],\n",
    "    \"leaf_estimation_method\": [\"Newton\", \"Gradient\"],\n",
    "    \"thread_count\":[-1],\n",
    "    \"verbose\":[0],\n",
    "    \"task_type\":[\"CPU\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data\n",
    "df_train_shuffled = df_train.sample(frac=1) # 1 means 100% of the data gets shuffled\n",
    "\n",
    "# Split into X & y\n",
    "X = df_train_shuffled.drop('Loan_Status', axis = 1)\n",
    "y = df_train_shuffled[\"Loan_Status\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_clf_CBC = GridSearchCV(estimator=clf_CBC,\n",
    "                            param_grid=grid_CBC_2,\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_clf_CBC.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_CBC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "# Make predictions with the best hyperparameters\n",
    "gs_y_preds_CBC = gs_clf_CBC.predict(X_valid)\n",
    "\n",
    "# Evaluate the predictions\n",
    "gs_metrics_CBC = evaluate_preds(y_valid, gs_y_preds_CBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_CBC, rs_metrics_CBC, gs_metrics_CBC, title=\"Comparison of CatBooster Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to predict the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silahkan gunakan model ini dengan data terbaru (df_test)\n",
    "\n",
    "## `gs_clf_(estimators).predict(df_test)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5SjvlWsSum3"
   },
   "source": [
    "# Model Selection | Pemilihan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6HA4aUfSu6-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the model names and evaluation metrics stored in dictionaries\n",
    "model_metrics = {\n",
    "    \"LinearSVC\"               : gs_metrics_LSVC,\n",
    "    \"Random Forest Classifier\": gs_metrics_RFC,\n",
    "    \"KNeighbors Classifier\"   : gs_metrics_KNC,\n",
    "    \"Logistic Regression\"     : gs_metrics_LR,\n",
    "    \"Decision Tree\"           : gs_metrics_tree,\n",
    "    \"XGBoost\"                 : gs_metrics_XGB,\n",
    "    \"CatBoost Classifier\"     : gs_metrics_CBC,\n",
    "    # Add more models and their evaluation metrics as needed\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "hehohi = pd.DataFrame.from_dict(model_metrics, orient='index')\n",
    "\n",
    "# Round the evaluation metrics to two decimal places\n",
    "hehohi = hehohi.round(2)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(hehohi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryln4XIEaR-Z"
   },
   "source": [
    "# Conclusion | Kesimpulan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah melakukan eksplorasi data, analisa, manipulasi, training model, tuning, dan mengevaluasi 7 model data yang berbeda, maka apabila dibandingkan menggunakan evaluasi matrix klasifikasi, yaitu akurasi, presisi, recall, dan f1 score, maka peneliti akan memilih KNeighbors Classifier sebagai estimator dalam model. Hal ini dikarenakan nilai evaluation metrics yang dimiliki cukup tinggi dan juga tidak terlalu jauh perbedaan persentase antar metrics.\n",
    "\n",
    "Kesimpulannya, KNeighbours merupakan model yang cocok untuk memprediksi eligibilitas nasabah yang melakukan peminjaman ke bank karena akurasinya yang tinggi. Hal ini dapat lebih dioptimalkan dan diterapkan untuk aplikasi dunia nyata dalam domain perbankan."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
